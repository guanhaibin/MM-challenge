# MM-challenge
Multiple Myeloma Challenge 

Used stacking to create an ensemble of several baseline classifiers for constructing a precise risk stratification model to identify high risk Multiple Myeloma patients using gene expression variables with ISS and age.  

Given that the four training data sets: GSE24080UAMS, HOVON65, EMTAB and MMRF, applied the same pipeline for each of them generating separate sets of models for the four data sets. 

Resampling: Since the data sets were unbalanced in terms of the number of high risk versus the number of low risk subjects, we used an algorithm that creates datasets with balanced distributions by combining oversampling by SMOTE and undersampling by Tomek [Batista et al., 2003]. 

Feature Selection: Next, the size of the dataset was reduced by filtering out genes that were not differentially expressed. The R Bioconductor limma package was used for assessing differential gene expression and returned a ranking of the genes. To further perform feature selection, we utilized SVM-RFECV and MRMR approaches [Guyon et al., 2002]. 
To apply the first method, a support vector machine (SVM) classifier with a linear kernel was trained using all the features. Then, recursive feature elimination (RFE) was applied by using the weight magnitude as a ranking criterion. Smaller subsets (for backward feature elimination) of features were considered by removing lowest ranked features first and cross-validation (CV) was used to determine the optimal number of features. The idea of the second method, Minimum Redundancy and Maximum Relevance (MRMR), is to select a subset of genes so that each gene in the subset has the highest similarity with target vector (maximum relevance), while simultaneously having the lowest similarity with the rest of the genes in the subset (minimum redundancy). Mutual information was used as a measure of similarity. Note that each training data set may have different set of relevant/redundant genes. The process generated 29 genes for the GSE24080UAMS set, 638 genes for the HOVON65 set, 1314 genes for the Kryukov set, and 41 for the MMRF set.

Baseline Classifiers: For each training data set, the selected genes were used to train six baseline classifiers. The baseline classifiers included Support Vector Machine (SVM), Neural Network (NN), Random Forest (RF), Gradient Boosting Machine (GBM), Learning Vector Quantization (LVQ) and Generalized Linear Model (GLM). The parameters of all baseline models were optimized with cross validation. Finally, we applied stacking by training ensemble classifiers to combine the results of the abovementioned baseline classifiers [Dzeroski and Zenko, 2004]. The basic idea of stacking is to train machine learning algorithms with training dataset and then generate a new dataset with these models. Then this new dataset is used as an input for the combined machine learning algorithm. Finally, a neural network learning algorithm that uses weighted voting is used as an ensemble classifier. 

Classifiers for Group Membership: In order to figure out the right model to use, we need to first decide which group a given test data sample belongs to. We therefore trained classifiers to perform this task, where the four training sets, GSE24080UAMS, HOVON65, EMTAB and MMRF, were used to train. 

Classifying Test Data: Any missing data in the test data sample was imputed with averaging. Next, the sample is classified into the right group using the group membership classifier mentioned above. Depending on the classification, the appropriate set of classifiers (baseline classifiers followed by the stacking method) are applied to the sample to classify its risk level. 
Software Tools: All analysis was performed with R package caret, which in turn uses FCNN4R for NN, randomForest for RF, gbm for Gradient Boosting Machine, glm for Generalized Linear Model, e1071 for Support Vector Machines, and class for Learning Vector Quantization. 
